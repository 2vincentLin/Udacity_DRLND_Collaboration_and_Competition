{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity DRLND Collaboration and Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "The choice of algorithm to solve the Unity Tennis environment is [DDPG](https://arxiv.org/abs/1509.02971). Both players use different replay buffer.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "    BUFFER_SIZE = int(1e6)  # replay buffer size  \n",
    "    BATCH_SIZE = 256        # minibatch size  \n",
    "    GAMMA = 0.9             # discount factor  \n",
    "    TAU = 1e-3              # for soft update of target parameters  \n",
    "    LR_ACTOR = 1e-4         # learning rate of the actor  \n",
    "    LR_CRITIC = 1e-4        # learning rate of the critic  \n",
    "    WEIGHT_DECAY = 0.0001   # L2 weight decay\n",
    "\n",
    "### Model architecture\n",
    "\n",
    "For this project, I used a slightly simple neural network, so using GPU does not actually speed up much compared to training on CPU.\n",
    "\n",
    "Actor-network\n",
    "\n",
    "| <i></i>  |input units|output units|\n",
    "|---|---|---|\n",
    "|input layer|24|128|\n",
    "|hidden layer|128|128|\n",
    "|output layer|128|2|\n",
    "\n",
    "Critic-network\n",
    "\n",
    "| <i></i>  |input units|output units|\n",
    "|---|---|---|\n",
    "|input layer|24|128|\n",
    "|hidden layer|128+2|128|\n",
    "|output layer|128|1|\n",
    "\n",
    "I also add batch normalization layer on most layers except before input and after output layer of Critic-network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. load the necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "%aimport ddpg_agents\n",
    "from ddpg_agents import Agent\n",
    "from model import Actor, Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "states shape:  (2, 24)\n"
     ]
    }
   ],
   "source": [
    "file = \"Tennis_Windows_x86_64/Tennis.exe\"\n",
    "env = UnityEnvironment(file_name=file)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print('states shape: ', states.shape)\n",
    "state_size = states.shape[1]\n",
    "full_state_size = num_agents * state_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. training control\n",
    "\n",
    "Due to the long training time, I come up with a simple idea to be able to stop and continue the training without losing any progress. I create a python dictionary called log to store some hyperparameters like total_episodes, etc. Then I use another python script called log_control to check and modify log. To be able to stop the training anytime I want, there is an item whose key called 'end_now', if I want to end the training, I simply call end_now() in log_control.py, then before the next episode, ddpg() will check if it's True, if yes, then ddpg() ends the training right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "log = {\n",
    "    'total_episodes': 3000,\n",
    "    'current_episodes': 0,\n",
    "    'scores': [],\n",
    "    'save_every': 100,\n",
    "    'print_every': 100,\n",
    "    'end_now': False,\n",
    "    'solved': False,\n",
    "    'solved_score':0.5,\n",
    "    'solve_in_episodes': None\n",
    "}\n",
    "\n",
    "import os.path\n",
    "log_path = 'log.pkl'\n",
    "\n",
    "def save_log(log_path, log):\n",
    "    with open(log_path, 'wb') as f: \n",
    "        pickle.dump(log, f)\n",
    "def load_log(log_path):\n",
    "    with open(log_path, 'rb') as f: \n",
    "        log = pickle.load(f)\n",
    "    return log\n",
    "\n",
    "# uncomment the following line if you need to restart\n",
    "# save_log(log_path, log)\n",
    "\n",
    "if os.path.exists(log_path):\n",
    "    log = load_log(log_path)\n",
    "else:\n",
    "    save_log(log_path, log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.0040\tScore: 0.0000\n",
      "Episode 200\tAverage Score: 0.0070\tScore: 0.0000\n",
      "Episode 300\tAverage Score: 0.0130\tScore: 0.0000\n",
      "Episode 400\tAverage Score: 0.0050\tScore: 0.0000\n",
      "Episode 500\tAverage Score: 0.0160\tScore: 0.0000\n",
      "Episode 600\tAverage Score: 0.0110\tScore: 0.0000\n",
      "Episode 700\tAverage Score: 0.0110\tScore: 0.0000\n",
      "Episode 800\tAverage Score: 0.0380\tScore: 0.1000\n",
      "Episode 900\tAverage Score: 0.0620\tScore: 0.1000\n",
      "Episode 1000\tAverage Score: 0.0880\tScore: 0.1000\n",
      "Episode 1100\tAverage Score: 0.0770\tScore: 0.0000\n",
      "Episode 1200\tAverage Score: 0.0940\tScore: 0.0000\n",
      "Episode 1300\tAverage Score: 0.0760\tScore: 0.1000\n",
      "Episode 1400\tAverage Score: 0.1090\tScore: 0.2000\n",
      "Episode 1500\tAverage Score: 0.1210\tScore: 0.1000\n",
      "Episode 1600\tAverage Score: 0.1038\tScore: 0.1000\n",
      "Episode 1700\tAverage Score: 0.1090\tScore: 0.1000\n",
      "Episode 1800\tAverage Score: 0.1190\tScore: 0.1000\n",
      "Episode 1900\tAverage Score: 0.1490\tScore: 0.1000\n",
      "Episode 2000\tAverage Score: 0.2210\tScore: 0.1000\n",
      "Episode 2100\tAverage Score: 0.2078\tScore: 0.7000\n",
      "Episode 2200\tAverage Score: 0.2329\tScore: 0.2000\n",
      "Episode 2272\tAverage Score: 0.5200\tScore: 4.5000\n",
      "environment solved in 2272th episodes\n",
      "Episode 2300\tAverage Score: 0.8189\tScore: 1.4000\n",
      "Episode 2400\tAverage Score: 1.9549\tScore: 2.9000\n",
      "Episode 2500\tAverage Score: 1.8148\tScore: 2.3000\n",
      "Episode 2600\tAverage Score: 1.8730\tScore: 2.1000\n",
      "Episode 2700\tAverage Score: 1.8040\tScore: 0.8000\n",
      "Episode 2800\tAverage Score: 1.7540\tScore: 1.4000\n",
      "Episode 2900\tAverage Score: 2.0230\tScore: 0.1000\n",
      "Episode 3000\tAverage Score: 0.8380\tScore: 0.6000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6h 9min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# agent = Agent(state_size=state_size, full_state_size=full_state_size, action_size=action_size, \n",
    "#               num_agents=num_agents, num_process=2, random_seed=0)\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, num_process=2, random_seed=0)\n",
    "\n",
    "def ddpg(max_t=1000, log=log, log_path=log_path):\n",
    "    # ----------load variables\n",
    "    total_episodes = log['total_episodes']\n",
    "    current_episodes = log['current_episodes']\n",
    "    scores = log['scores']\n",
    "    save_every = log['save_every']\n",
    "    print_every = log['print_every']\n",
    "    solved = log['solved']\n",
    "    solved_score = log['solved_score']\n",
    "    scores_deque = deque(maxlen=100)\n",
    "\n",
    "    # ----------load weights if continue training\n",
    "    if current_episodes != 0:\n",
    "        agent.load_weights()\n",
    "\n",
    "    for i_episode in range(current_episodes + 1, total_episodes + 1):\n",
    "\n",
    "        # -----check if need to end the training-------\n",
    "        log = load_log(log_path)\n",
    "        if log['end_now']:\n",
    "            log['end_now'] = False\n",
    "            save_log(log_path, log)\n",
    "            break\n",
    "\n",
    "        # -----generating state--------\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment    \n",
    "        states = env_info.vector_observations  # get the current state (for each agent)\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)\n",
    "            env_info = env.step(actions)[brain_name]  # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations  # get next state (for each agent)\n",
    "            rewards = env_info.rewards  # get reward (for each agent)\n",
    "            dones = env_info.local_done  # see if episode finished\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "            states = next_states\n",
    "            score += np.max(rewards)\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.4f}\\tScore: {:.4f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.4f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) > solved_score and solved == False:\n",
    "            print('\\nenvironment solved in {}th episodes'.format(i_episode))\n",
    "            solved = True\n",
    "            log['solved'] = True\n",
    "            agent.save_weights()\n",
    "            save_log(log_path, log)\n",
    "\n",
    "        if i_episode % save_every == 0:\n",
    "            log['current_episodes'] = i_episode\n",
    "            log['scores'] = scores\n",
    "            save_log(log_path, log)\n",
    "            agent.save_weights()\n",
    "    if np.mean(scores_deque) > solved_score:\n",
    "        agent.save_weights()\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.9000000134110451\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, num_process=2, random_seed=0)\n",
    "agent.load_weights()\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = agent.act(states)                        # select an action (for each agent)\n",
    "\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += np.max(env_info.rewards)                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future improve\n",
    "\n",
    "First, I tried to implement maddpg to solve the tennis environment, however, the agent learned nothing after 30k episodes(the rewards is 0 for most of the times). However, when I checked the loss, the algorithm did reduce the loss. That means one thing, the critic couldn't give good advice of all time. Consider the only change I did to the model architecture of critic is to change state and action dimension to full state and action dimension. And the way I get full state space is to simply concatenate two states space from both players, I think this is the reason the critic learned nothing. For one reason, both state space contains the position and velocity of the ball, but it seems they are additive inverse. However, without the detailed knowledge of state space, I couldn't get a full state space without redudent information.\n",
    "\n",
    "Second, although ddpg successfully solved this environment, it seems that the performance of the agent stated to drop after 2900th episodes.\n",
    "\n",
    "So, there are some future works for me to explore and improve my agent.\n",
    "\n",
    "1. trying to get full state space without redudent information, in other words, make maddpg works. \n",
    "\n",
    "\tThe reason for maddpg is better than ddpg in this environment is if you check the replay. ddpg agent only knows its own environment, it wouldn't know if its opponent was unable to hit the ball(foul ball). But for maddpg agent, at least it has information of full state thus potentially knowing how to deliver the ball.  \n",
    "    \n",
    "\n",
    "2. keep digging the limitation of the performance of ddpg agent in this environment. Though ddpg has no info on its opponent, however, given enough episodes, I believe it should be able to learn how to deliver the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
